{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Lake features"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from delta.tables import DeltaTable\r\n",
        "\r\n",
        "manualSchema = StructType([\r\n",
        "  StructField(\"CustomerId\", StringType(), True),\r\n",
        "  StructField(\"ProductId\", StringType(), True),\r\n",
        "  StructField(\"Rating\", LongType(), True),\r\n",
        "  StructField(\"Cost\", FloatType(), True),\r\n",
        "  StructField(\"Size\", FloatType(), True),\r\n",
        "  StructField(\"Price\", FloatType(), True),\r\n",
        "  StructField(\"PrimaryBrandId\", LongType(), True),\r\n",
        "  StructField(\"GenderId\", LongType(), True),\r\n",
        "  StructField(\"MaritalStatus\", LongType(), True),\r\n",
        "  StructField(\"LowerIncomeBound\", FloatType(), True),\r\n",
        "  StructField(\"UpperIncomeBound\", FloatType(), True)\r\n",
        "])\r\n",
        "\r\n",
        "url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\r\n",
        "raw_data = spark.read.csv(url, header=True, schema=manualSchema)\r\n",
        "print(\"Schema: \")\r\n",
        "raw_data.printSchema()\r\n",
        "\r\n",
        "df = raw_data.toPandas()\r\n",
        "print(\"Shape: \", df.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table_path = 'abfss://delta@asadatalake20220206.dfs.core.windows.net/customer-rating'\r\n",
        "\r\n",
        "raw_data.write.format('delta').save(delta_table_path)\r\n",
        "\r\n",
        "mssparkutils.fs.ls(delta_table_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_log_path = mssparkutils.fs.ls(f'{delta_table_path}/_delta_log')[0].path\r\n",
        "print(delta_log_path)\r\n",
        "mssparkutils.fs.head(delta_log_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.format('delta').load(delta_table_path)\r\n",
        "data.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all versions\r\n",
        "delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
        "display(delta_table.history())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the predicate by using a SQL-formatted string.\r\n",
        "delta_table.update(\r\n",
        "  condition = \"Price < 1500\",\r\n",
        "  set = { \"Price\": \"Price * 1.05\" }\r\n",
        ")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(delta_table.history())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's possible to query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_table_path))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(delta_table_path))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE TABLE CustomerRating USING DELTA LOCATION '{0}'\".format(delta_table_path))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"DESCRIBE EXTENDED customerrating\").show(truncate=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To query the delta table from the serverless SQL pool, navigate to the `Develop` hub in Synapse Studio and create a new SQL script. Make sure `Built-in` is selected for the `Connect to` option and `default` is selected for the `Use database` option.\r\n",
        "\r\n",
        "Enter the query as shown in the picture below and make sure you replace the name of the Data Lake account with the one from your lab environment.\r\n",
        "\r\n",
        "![Query Delta Lake with serverless SQL pool](https://solliancepublicdata.blob.core.windows.net/synapse-l400/notebook-images/query-delta-table.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This concludes the Delta Lake section of this notebook.\r\n",
        "\r\n",
        "To learn more about Delta Lake support in Syanspe Spark, take a look at the [Work with Delta Lake](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-delta-lake-overview?pivots=programming-language-python) section in the Azure Synapse Analytics documentation."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}